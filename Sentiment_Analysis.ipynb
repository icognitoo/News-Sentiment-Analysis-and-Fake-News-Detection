{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGzfQwoEieg3AP2K1yUlvr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Wl2Ls7FOtf75",
        "outputId": "b03ad05d-7648-404c-ad26-e07067870fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.12/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.48.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.12/dist-packages (1.9.4)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from wordcloud) (11.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.14.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Environment setup complete! ✅\n",
            "TensorFlow version: 2.19.0\n",
            "PyTorch version: 2.8.0+cu126\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Using sample data for demonstration...\n",
            "Created 5 fake news samples and 5 sentiment samples\n",
            "Applying preprocessing to datasets...\n",
            "Sentiment data - Train: 3, Val: 1, Test: 1\n",
            "Fake news data - Train: 3, Val: 1, Test: 1\n",
            "Vocabulary size: 21\n",
            "Sequence length: 150\n",
            "Training LSTM_Sentiment...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1351506574.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;31m# Train sentiment models (Example: LSTM, BiLSTM, Hybrid CNN-BiLSTM)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0mlstm_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_architectures_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_sentiment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'LSTM_Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0mbilstm_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_architectures_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_bilstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_prep_sentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1351506574.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(self, model, X_train, y_train, X_val, y_val, model_name, epochs, batch_size)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         history = model.fit(\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/backend/tensorflow/nn.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    777\u001b[0m             \u001b[0;34m\"Arguments `target` and `output` must have the same rank \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;34m\"(ndim). Received: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Arguments `target` and `output` must have the same rank (ndim). Received: target.shape=(None,), output.shape=(None, 3)"
          ]
        }
      ],
      "source": [
        "# Environment Setup and Library Installation\n",
        "!pip install transformers torch torchvision torchaudio\n",
        "!pip install datasets\n",
        "!pip install lime shap\n",
        "!pip install nltk spacy\n",
        "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install wordcloud\n",
        "!pip install tensorflow\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab') # Added missing resource download\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import nltk\n",
        "import spacy\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.tag import pos_tag\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding, Dropout, GlobalMaxPooling1D, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import shap\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"Environment setup complete! ✅\")\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
        "\n",
        "# Data Acquisition and Processing\n",
        "def create_sample_data():\n",
        "    fake_news_data = {\n",
        "        'title': [\n",
        "            'Breaking: Scientists discover cure for all diseases',\n",
        "            'Government secretly controlling weather with satellites',\n",
        "            'Celebrity endorses miracle weight loss pill',\n",
        "            'Local mayor caught in corruption scandal',\n",
        "            'Stock market manipulation exposed by whistleblower'\n",
        "        ],\n",
        "        'text': [\n",
        "            'In a groundbreaking discovery that will change medicine forever, researchers claim to have found a universal cure...',\n",
        "            'Leaked documents reveal government weather control program using advanced satellite technology...',\n",
        "            'Popular celebrity claims this one pill helped them lose 50 pounds in just two weeks...',\n",
        "            'Investigation reveals mayor received kickbacks from construction companies for city contracts...',\n",
        "            'Former trader reveals how major banks manipulate stock prices through coordinated trading...'\n",
        "        ],\n",
        "        'label': [1, 1, 1, 0, 0]  # 1 = fake, 0 = real\n",
        "    }\n",
        "\n",
        "    sentiment_data = {\n",
        "        'text': [\n",
        "            'I absolutely love this new technology! It\\'s amazing!',\n",
        "            'This product is terrible, waste of money.',\n",
        "            'The weather is okay today, nothing special.',\n",
        "            'Outstanding performance by the team today!',\n",
        "            'Not sure how I feel about this decision.'\n",
        "        ],\n",
        "        'sentiment': ['positive', 'negative', 'neutral', 'positive', 'neutral']\n",
        "    }\n",
        "\n",
        "    return pd.DataFrame(fake_news_data), pd.DataFrame(sentiment_data)\n",
        "\n",
        "try:\n",
        "    import os\n",
        "    if not os.path.exists('WELFake_Dataset.csv'):\n",
        "        !wget -q https://zenodo.org/record/4561253/files/WELFake_Dataset.csv\n",
        "\n",
        "    if not os.path.exists('train.csv'):\n",
        "        !wget -q https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\n",
        "\n",
        "    fake_news_df = pd.read_csv('WELFake_Dataset.csv')\n",
        "    sentiment_df = pd.read_csv('train.csv', encoding='latin-1', header=None)\n",
        "    sentiment_df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
        "    print(f\"Loaded {len(fake_news_df)} fake news samples and {len(sentiment_df)} sentiment samples\")\n",
        "except Exception:\n",
        "    print(\"Using sample data for demonstration...\")\n",
        "    fake_news_df, sentiment_df = create_sample_data()\n",
        "    print(f\"Created {len(fake_news_df)} fake news samples and {len(sentiment_df)} sentiment samples\")\n",
        "\n",
        "# Text Preprocessing Pipeline\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stemmer = PorterStemmer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
        "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "        text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def remove_stopwords(self, tokens):\n",
        "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
        "\n",
        "    def lemmatize_tokens(self, tokens):\n",
        "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "    def stem_tokens(self, tokens):\n",
        "        return [self.stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    def extract_named_entities(self, text):\n",
        "        doc = self.nlp(text)\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        return entities\n",
        "\n",
        "    def preprocess_text(self, text, include_entities=True, use_lemmatization=True):\n",
        "        cleaned = self.clean_text(text)\n",
        "        tokens = self.tokenize_text(cleaned)\n",
        "        tokens = self.remove_stopwords(tokens)\n",
        "        tokens = self.lemmatize_tokens(tokens) if use_lemmatization else self.stem_tokens(tokens)\n",
        "\n",
        "        entities = self.extract_named_entities(cleaned) if include_entities else []\n",
        "\n",
        "        return {\n",
        "            'processed_text': ' '.join(tokens),\n",
        "            'tokens': tokens,\n",
        "            'entities': entities,\n",
        "            'original_length': len(text.split()),\n",
        "            'processed_length': len(tokens),\n",
        "        }\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "\n",
        "# Apply preprocessing to datasets\n",
        "print(\"Applying preprocessing to datasets...\")\n",
        "\n",
        "if 'text' in fake_news_df.columns:\n",
        "    fake_news_df['processed_text'] = fake_news_df['text'].apply(\n",
        "        lambda x: preprocessor.preprocess_text(str(x))['processed_text']\n",
        "    )\n",
        "\n",
        "sentiment_df['processed_text'] = sentiment_df['text'].apply(\n",
        "    lambda x: preprocessor.preprocess_text(str(x))['processed_text']\n",
        ")\n",
        "\n",
        "# Neural Network Architectures\n",
        "class NeuralArchitectures:\n",
        "    def __init__(self, vocab_size=10000, max_length=100, embedding_dim=128):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "    def create_lstm_model(self, num_classes=3, lstm_units=64):\n",
        "        model = Sequential([\n",
        "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
        "            LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def create_bilstm_model(self, num_classes=3, lstm_units=64):\n",
        "        model = Sequential([\n",
        "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
        "            Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3)),\n",
        "            Dense(128, activation='relu'),\n",
        "            Dropout(0.5),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.3),\n",
        "            Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
        "        ])\n",
        "        return model\n",
        "\n",
        "    def create_cnn_model(self, num_classes=3, filters=64, kernel_sizes=[3,4,5]):\n",
        "        inputs = Input(shape=(self.max_length,))\n",
        "        embedding = Embedding(self.vocab_size, self.embedding_dim)(inputs)\n",
        "        conv_blocks = []\n",
        "        for kernel_size in kernel_sizes:\n",
        "            conv = Conv1D(filters, kernel_size, activation='relu')(embedding)\n",
        "            conv = GlobalMaxPooling1D()(conv)\n",
        "            conv_blocks.append(conv)\n",
        "        concat = tf.keras.layers.concatenate(conv_blocks)\n",
        "        dense = Dense(128, activation='relu')(concat)\n",
        "        dropout = Dropout(0.5)(dense)\n",
        "        outputs = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(dropout)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "    def create_hybrid_cnn_bilstm_model(self, num_classes=3, cnn_filters=64, lstm_units=64):\n",
        "        inputs = Input(shape=(self.max_length,))\n",
        "        embedding = Embedding(self.vocab_size, self.embedding_dim)(inputs)\n",
        "        conv1 = Conv1D(cnn_filters, 3, activation='relu')(embedding)\n",
        "        conv1 = MaxPooling1D(2)(conv1)\n",
        "        conv2 = Conv1D(cnn_filters, 4, activation='relu')(conv1)\n",
        "        conv2 = MaxPooling1D(2)(conv2)\n",
        "        lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.3))(conv2)\n",
        "        lstm = Bidirectional(LSTM(lstm_units//2, dropout=0.3))(lstm)\n",
        "        dense = Dense(128, activation='relu')(lstm)\n",
        "        dropout = Dropout(0.5)(dense)\n",
        "        dense2 = Dense(64, activation='relu')(dropout)\n",
        "        dropout2 = Dropout(0.3)(dense2)\n",
        "        outputs = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(dropout2)\n",
        "        model = Model(inputs=inputs, outputs=outputs)\n",
        "        return model\n",
        "\n",
        "\n",
        "# Data Preparation and Feature Engineering\n",
        "class DataPreparation:\n",
        "    def __init__(self, vocab_size=10000, max_length=100):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = None\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def prepare_text_data(self, texts, labels=None, fit_tokenizer=True):\n",
        "        if fit_tokenizer or self.tokenizer is None:\n",
        "            self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')\n",
        "            self.tokenizer.fit_on_texts(texts)\n",
        "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "        X = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
        "        y = None\n",
        "        if labels is not None:\n",
        "            # Fit LabelEncoder only if it hasn't been fitted yet or if fit_tokenizer is True\n",
        "            if fit_tokenizer or not hasattr(self.label_encoder, 'classes_'):\n",
        "                self.label_encoder.fit(labels)\n",
        "            y = self.label_encoder.transform(labels)\n",
        "        return X, y\n",
        "\n",
        "    def create_train_test_split(self, X, y, test_size=0.2, validation_size=0.1, stratify=None): # Modified to accept stratify\n",
        "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=42, stratify=stratify # Modified to use stratify argument\n",
        "        )\n",
        "        val_size_adjusted = validation_size / (1 - test_size)\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=stratify # Modified to use stratify argument\n",
        "        )\n",
        "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "# Data Preparation for Sentiment Analysis\n",
        "data_prep_sentiment = DataPreparation(vocab_size=10000, max_length=150)\n",
        "sentiment_texts = sentiment_df['processed_text'].fillna('').tolist()\n",
        "sentiment_labels = sentiment_df['sentiment'].tolist() if 'sentiment' in sentiment_df.columns else ['neutral'] * len(sentiment_texts)\n",
        "X_sentiment, y_sentiment = data_prep_sentiment.prepare_text_data(sentiment_texts, sentiment_labels, fit_tokenizer=True)\n",
        "\n",
        "# Check if using sample data and adjust stratify accordingly\n",
        "if len(sentiment_df) <= 5: # Assuming sample data has 5 rows or less\n",
        "    X_train_sent, X_val_sent, X_test_sent, y_train_sent, y_val_sent, y_test_sent = data_prep_sentiment.create_train_test_split(\n",
        "        X_sentiment, y_sentiment, stratify=None # Set stratify to None for sample data\n",
        "    )\n",
        "else:\n",
        "     X_train_sent, X_val_sent, X_test_sent, y_train_sent, y_val_sent, y_test_sent = data_prep_sentiment.create_train_test_split(\n",
        "        X_sentiment, y_sentiment, stratify=y_sentiment # Use stratify for larger datasets\n",
        "    )\n",
        "\n",
        "\n",
        "# Data Preparation for Fake News Detection\n",
        "data_prep_fake = DataPreparation(vocab_size=data_prep_sentiment.vocab_size, max_length=data_prep_sentiment.max_length)\n",
        "data_prep_fake.tokenizer = data_prep_sentiment.tokenizer # Use the same tokenizer\n",
        "if 'processed_text' in fake_news_df.columns:\n",
        "    fake_texts = fake_news_df['processed_text'].fillna('').tolist()\n",
        "    fake_labels = fake_news_df['label'].tolist() if 'label' in fake_news_df.columns else [0] * len(fake_texts)\n",
        "\n",
        "    X_fake, y_fake = data_prep_fake.prepare_text_data(fake_texts, fake_labels, fit_tokenizer=False)\n",
        "\n",
        "    # Check if using sample data and adjust stratify accordingly\n",
        "    if len(fake_news_df) <= 5: # Assuming sample data has 5 rows or less\n",
        "        X_train_fake, X_val_fake, X_test_fake, y_train_fake, y_val_fake, y_test_fake = data_prep_fake.create_train_test_split(\n",
        "            X_fake, y_fake, stratify=None # Set stratify to None for sample data\n",
        "        )\n",
        "    else:\n",
        "        X_train_fake, X_val_fake, X_test_fake, y_train_fake, y_val_fake, y_test_fake = data_prep_fake.create_train_test_split(\n",
        "            X_fake, y_fake, stratify=y_fake # Use stratify for larger datasets\n",
        "        )\n",
        "\n",
        "else:\n",
        "    print(\"Fake news data not available, using sample data\")\n",
        "    X_train_fake = X_val_fake = X_test_fake = np.array([[1, 2, 3]])\n",
        "    y_train_fake = y_val_fake = y_test_fake = np.array([0])\n",
        "\n",
        "\n",
        "print(f\"Sentiment data - Train: {X_train_sent.shape[0]}, Val: {X_val_sent.shape[0]}, Test: {X_test_sent.shape[0]}\")\n",
        "print(f\"Fake news data - Train: {X_train_fake.shape[0]}, Val: {X_val_fake.shape[0]}, Test: {X_test_fake.shape[0]}\")\n",
        "print(f\"Vocabulary size: {len(data_prep_sentiment.tokenizer.word_index)}\")\n",
        "print(f\"Sequence length: {data_prep_sentiment.max_length}\")\n",
        "\n",
        "# Model Training with Callbacks\n",
        "class ModelTrainer:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.trained_models = {}\n",
        "\n",
        "    def get_callbacks(self, patience=5, min_delta=0.001):\n",
        "        return [\n",
        "            EarlyStopping(monitor='val_loss', patience=patience, min_delta=min_delta, restore_best_weights=True, verbose=1),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
        "        ]\n",
        "\n",
        "    def train_model(self, model, X_train, y_train, X_val, y_val, model_name='model', epochs=50, batch_size=32):\n",
        "        print(f\"Training {model_name}...\")\n",
        "        num_classes = len(np.unique(y_train))\n",
        "        loss_fn = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'\n",
        "        metrics = ['accuracy']\n",
        "        model.compile(optimizer='adam', loss=loss_fn, metrics=metrics)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=self.get_callbacks(),\n",
        "            verbose=1\n",
        "        )\n",
        "        self.history[model_name] = history\n",
        "        self.trained_models[model_name] = model\n",
        "        print(f\"{model_name} training completed!\")\n",
        "        return history\n",
        "\n",
        "    def plot_training_history(self, model_names=None):\n",
        "        if model_names is None:\n",
        "            model_names = list(self.history.keys())\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
        "\n",
        "        for name in model_names:\n",
        "            h = self.history[name]\n",
        "            axs[0].plot(h.history['accuracy'], label=f\"{name} Train\")\n",
        "            axs[0].plot(h.history['val_accuracy'], label=f\"{name} Val\")\n",
        "            axs[1].plot(h.history['loss'], label=f\"{name} Train\")\n",
        "            axs[1].plot(h.history['val_loss'], label=f\"{name} Val\")\n",
        "        axs[0].set_title(\"Accuracy\")\n",
        "        axs[0].legend()\n",
        "        axs[1].set_title(\"Loss\")\n",
        "        axs[1].legend()\n",
        "        plt.show()\n",
        "\n",
        "trainer = ModelTrainer()\n",
        "nn_architectures_sentiment = NeuralArchitectures(vocab_size=data_prep_sentiment.vocab_size, max_length=data_prep_sentiment.max_length)\n",
        "nn_architectures_fake = NeuralArchitectures(vocab_size=data_prep_fake.vocab_size, max_length=data_prep_fake.max_length)\n",
        "\n",
        "\n",
        "# Train sentiment models (Example: LSTM, BiLSTM, Hybrid CNN-BiLSTM)\n",
        "lstm_sentiment = nn_architectures_sentiment.create_lstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
        "trainer.train_model(lstm_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='LSTM_Sentiment', epochs=10, batch_size=32)\n",
        "\n",
        "bilstm_sentiment = nn_architectures_sentiment.create_bilstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
        "trainer.train_model(bilstm_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='BiLSTM_Sentiment', epochs=10, batch_size=32)\n",
        "\n",
        "hybrid_sentiment = nn_architectures_sentiment.create_hybrid_cnn_bilstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
        "trainer.train_model(hybrid_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='Hybrid_CNN_BiLSTM', epochs=10, batch_size=32)\n",
        "\n",
        "trainer.plot_training_history(model_names=['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM'])\n",
        "\n",
        "# Train fake news models (Example: LSTM, BiLSTM, Hybrid CNN-BiLSTM)\n",
        "lstm_fake = nn_architectures_fake.create_lstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
        "trainer.train_model(lstm_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='LSTM_Fake', epochs=10, batch_size=32)\n",
        "\n",
        "bilstm_fake = nn_architectures_fake.create_bilstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
        "trainer.train_model(bilstm_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='BiLSTM_Fake', epochs=10, batch_size=32)\n",
        "\n",
        "hybrid_fake = nn_architectures_fake.create_hybrid_cnn_bilstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
        "trainer.train_model(hybrid_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='Hybrid_CNN_BiLSTM_Fake', epochs=10, batch_size=32)\n",
        "\n",
        "trainer.plot_training_history(model_names=['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake'])\n",
        "\n",
        "\n",
        "# Model Evaluation\n",
        "class ModelEvaluator:\n",
        "    def __init__(self):\n",
        "        self.evaluation_results = {}\n",
        "\n",
        "    def evaluate_model(self, model, X_test, y_test, model_name='model'):\n",
        "        print(f\"Evaluating {model_name}...\")\n",
        "        y_pred_proba = model.predict(X_test, verbose=0)\n",
        "        if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:\n",
        "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
        "            is_binary = False\n",
        "        else:\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
        "            is_binary = True\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        if is_binary:\n",
        "            precision = precision_score(y_test, y_pred)\n",
        "            recall = recall_score(y_test, y_pred)\n",
        "            f1 = f1_score(y_test, y_pred)\n",
        "        else:\n",
        "            precision = precision_score(y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(y_test, y_pred, average='weighted')\n",
        "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "        self.evaluation_results[model_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1,\n",
        "            'y_true': y_test,\n",
        "            'y_pred': y_pred,\n",
        "            'y_pred_proba': y_pred_proba,\n",
        "        }\n",
        "        print(f\"{model_name} evaluation completed!\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "        return self.evaluation_results[model_name]\n",
        "\n",
        "    def plot_confusion_matrices(self, model_names=None):\n",
        "        if model_names is None:\n",
        "            model_names = list(self.evaluation_results.keys())\n",
        "        n = len(model_names)\n",
        "        fig, axs = plt.subplots(1, n, figsize=(5*n, 4))\n",
        "        if n == 1:\n",
        "            axs = [axs]\n",
        "        for i, name in enumerate(model_names):\n",
        "            results = self.evaluation_results[name]\n",
        "            cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axs[i])\n",
        "            axs[i].set_title(f\"{name} Confusion Matrix\")\n",
        "            axs[i].set_xlabel(\"Predicted\")\n",
        "            axs[i].set_ylabel(\"Actual\")\n",
        "        plt.show()\n",
        "\n",
        "evaluator = ModelEvaluator()\n",
        "\n",
        "# Evaluate sentiment models\n",
        "for model_name in ['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM']:\n",
        "    evaluator.evaluate_model(trainer.trained_models[model_name], X_test_sent, y_test_sent, model_name)\n",
        "evaluator.plot_confusion_matrices(model_names=['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM'])\n",
        "\n",
        "# Evaluate fake news models\n",
        "for model_name in ['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake']:\n",
        "    evaluator.evaluate_model(trainer.trained_models[model_name], X_test_fake, y_test_fake, model_name)\n",
        "evaluator.plot_confusion_matrices(model_names=['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake'])\n",
        "\n",
        "\n",
        "# Explainable AI Implementation\n",
        "class ExplainableAI:\n",
        "    def __init__(self, tokenizer, label_encoder=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_encoder = label_encoder\n",
        "        self.lime_explainer = LimeTextExplainer(class_names=label_encoder.classes_ if label_encoder is not None else ['negative', 'neutral', 'positive'])\n",
        "\n",
        "    def create_prediction_function(self, model, max_length=150):\n",
        "        def predict_fn(texts):\n",
        "            sequences = self.tokenizer.texts_to_sequences(texts)\n",
        "            padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "            preds = model.predict(padded, verbose=0)\n",
        "            if len(preds.shape) == 1:\n",
        "                proba = np.array([1 - preds, preds]).T\n",
        "            else:\n",
        "                proba = preds\n",
        "            return proba\n",
        "        return predict_fn\n",
        "\n",
        "    def explain_prediction_lime(self, model, text, num_features=10, max_length=150):\n",
        "        # Re-initialize explainer with correct class names based on the model's task\n",
        "        class_names = self.label_encoder.classes_.tolist() if self.label_encoder is not None else ['negative', 'neutral', 'positive']\n",
        "        self.lime_explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "        predict_fn = self.create_prediction_function(model, max_length)\n",
        "        explanation = self.lime_explainer.explain_instance(text, predict_fn, num_features=num_features)\n",
        "        return explanation\n",
        "\n",
        "    def visualize_lime_explanation(self, explanation, save_path=None):\n",
        "        exp_list = explanation.as_list()\n",
        "        words = [item[0] for item in exp_list]\n",
        "        weights = [item[1] for item in exp_list]\n",
        "        colors = ['red' if w < 0 else 'green' for w in weights]\n",
        "\n",
        "        plt.figure(figsize=(10,6))\n",
        "        bars = plt.barh(words, weights, color=colors, alpha=0.7)\n",
        "        plt.xlabel('Feature Importance')\n",
        "        plt.title('LIME Explanation - Word Importance')\n",
        "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
        "\n",
        "        for bar, weight in zip(bars, weights):\n",
        "            plt.text(weight + (0.01 if weight >= 0 else -0.01), bar.get_y() + bar.get_height()/2,\n",
        "                     f'{weight:.3f}', ha='left' if weight >= 0 else 'right', va='center')\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "# Explainable AI for Sentiment Analysis\n",
        "explainer_sentiment = ExplainableAI(data_prep_sentiment.tokenizer, data_prep_sentiment.label_encoder)\n",
        "\n",
        "# Example usage for explanation for sentiment analysis\n",
        "sample_text_sentiment = \"The government has announced a new policy that positively impacts economic growth.\"\n",
        "sample_processed_sentiment = preprocessor.preprocess_text(sample_text_sentiment)['processed_text']\n",
        "explanation_sentiment = explainer_sentiment.explain_prediction_lime(trainer.trained_models['Hybrid_CNN_BiLSTM'], sample_processed_sentiment)\n",
        "explainer_sentiment.visualize_lime_explanation(explanation_sentiment)\n",
        "\n",
        "# Explainable AI for Fake News Detection\n",
        "explainer_fake = ExplainableAI(data_prep_fake.tokenizer, data_prep_fake.label_encoder)\n",
        "\n",
        "# Example usage for explanation for fake news detection\n",
        "sample_text_fake = \"Breaking: Scientists discover cure for all diseases\"\n",
        "sample_processed_fake = preprocessor.preprocess_text(sample_text_fake)['processed_text']\n",
        "explanation_fake = explainer_fake.explain_prediction_lime(trainer.trained_models['Hybrid_CNN_BiLSTM_Fake'], sample_processed_fake)\n",
        "explainer_fake.visualize_lime_explanation(explanation_fake)\n",
        "\n",
        "\n",
        "# Continuous Learning Mechanism\n",
        "class ContinuousLearning:\n",
        "    def __init__(self, base_model, tokenizer, label_encoder, learning_rate=0.0001):\n",
        "        self.base_model = base_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_encoder = label_encoder\n",
        "        self.learning_rate = learning_rate\n",
        "        self.update_history = []\n",
        "        self.performance_tracking = []\n",
        "\n",
        "    def incremental_update(self, new_texts, new_labels, epochs=3, batch_size=16, validation_split=0.2):\n",
        "        print(f\"Performing incremental update with {len(new_texts)} new samples...\")\n",
        "        sequences = self.tokenizer.texts_to_sequences(new_texts)\n",
        "        X_new = pad_sequences(sequences, maxlen=150, padding='post', truncating='post')\n",
        "        y_new = self.label_encoder.transform(new_labels)\n",
        "        self.base_model.optimizer.learning_rate = self.learning_rate\n",
        "        history = self.base_model.fit(\n",
        "            X_new, y_new,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            validation_split=validation_split,\n",
        "            verbose=1\n",
        "        )\n",
        "        update_info = {\n",
        "            'timestamp': pd.Timestamp.now(),\n",
        "            'num_samples': len(new_texts),\n",
        "            'final_loss': history.history['loss'][-1],\n",
        "            'final_accuracy': history.history['accuracy'][-1],\n",
        "        }\n",
        "        self.update_history.append(update_info)\n",
        "        print(\"Incremental update completed!\")\n",
        "        print(f\"Final Loss: {update_info['final_loss']:.4f}\")\n",
        "        print(f\"Final Accuracy: {update_info['final_accuracy']:.4f}\")\n",
        "        return history\n",
        "\n",
        "    def evaluate_performance_drift(self, X_test, y_test, threshold=0.05):\n",
        "        y_pred = np.argmax(self.base_model.predict(X_test, verbose=0), axis=1)\n",
        "        current_accuracy = accuracy_score(y_test, y_pred)\n",
        "        performance_info = {'timestamp': pd.Timestamp.now(), 'accuracy': current_accuracy}\n",
        "        self.performance_tracking.append(performance_info)\n",
        "        if len(self.performance_tracking) > 1:\n",
        "            previous_accuracy = self.performance_tracking[-2]['accuracy']\n",
        "            drift = abs(current_accuracy - previous_accuracy)\n",
        "            if drift > threshold:\n",
        "                print(f\"Performance drift detected: {drift:.4f}\")\n",
        "                return True, drift\n",
        "            else:\n",
        "                print(f\"Performance stable: {drift:.4f}\")\n",
        "                return False, drift\n",
        "        return False, 0.0\n",
        "\n",
        "# Continuous Learning for Sentiment Analysis\n",
        "continuous_learner_sentiment = ContinuousLearning(base_model=trainer.trained_models['Hybrid_CNN_BiLSTM'], tokenizer=data_prep_sentiment.tokenizer, label_encoder=data_prep_sentiment.label_encoder)\n",
        "\n",
        "# Example incremental learning simulation (with dummy new data for sentiment analysis)\n",
        "new_texts_sentiment = [\n",
        "    \"The economy is improving steadily with new policies.\",\n",
        "    \"Many people are skeptical about the government's new plan.\",\n",
        "    \"This is a bad decision and hurts citizens greatly.\"\n",
        "]\n",
        "new_processed_sentiment = [preprocessor.preprocess_text(text)['processed_text'] for text in new_texts_sentiment]\n",
        "new_labels_sentiment = ['positive', 'neutral', 'negative']\n",
        "\n",
        "continuous_learner_sentiment.incremental_update(new_processed_sentiment, new_labels_sentiment, epochs=2)\n",
        "\n",
        "# Continuous Learning for Fake News Detection\n",
        "continuous_learner_fake = ContinuousLearning(base_model=trainer.trained_models['Hybrid_CNN_BiLSTM_Fake'], tokenizer=data_prep_fake.tokenizer, label_encoder=data_prep_fake.label_encoder)\n",
        "\n",
        "# Example incremental learning simulation (with dummy new data for fake news detection)\n",
        "new_texts_fake = [\n",
        "    \"New study shows coffee cures cancer\",\n",
        "    \"Local hero saves cat from tree\",\n",
        "    \"Politician resigns amid scandal\"\n",
        "]\n",
        "new_processed_fake = [preprocessor.preprocess_text(text)['processed_text'] for text in new_texts_fake]\n",
        "new_labels_fake = [1, 0, 0] # 1 = fake, 0 = real\n",
        "\n",
        "continuous_learner_fake.incremental_update(new_processed_fake, new_labels_fake, epochs=2)\n",
        "\n",
        "\n",
        "print(\"Complete NLP system source code ready for execution in Google Colab.\")"
      ]
    }
  ]
}