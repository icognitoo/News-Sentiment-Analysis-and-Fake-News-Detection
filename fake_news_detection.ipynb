{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd70426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Setup and Library Installation\n",
    "!pip install transformers torch torchvision torchaudio\n",
    "!pip install datasets\n",
    "!pip install lime shap\n",
    "!pip install nltk spacy\n",
    "!pip install scikit-learn pandas numpy matplotlib seaborn\n",
    "!pip install wordcloud\n",
    "!pip install tensorflow\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab') # Added missing resource download\n",
    "\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.layers import Embedding, Dropout, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "import lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import shap\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(\"Environment setup complete! âœ…\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "\n",
    "# Data Acquisition and Processing\n",
    "def create_sample_data():\n",
    "    fake_news_data = {\n",
    "        'title': [\n",
    "            'Breaking: Scientists discover cure for all diseases',\n",
    "            'Government secretly controlling weather with satellites',\n",
    "            'Celebrity endorses miracle weight loss pill',\n",
    "            'Local mayor caught in corruption scandal',\n",
    "            'Stock market manipulation exposed by whistleblower'\n",
    "        ],\n",
    "        'text': [\n",
    "            'In a groundbreaking discovery that will change medicine forever, researchers claim to have found a universal cure...',\n",
    "            'Leaked documents reveal government weather control program using advanced satellite technology...',\n",
    "            'Popular celebrity claims this one pill helped them lose 50 pounds in just two weeks...',\n",
    "            'Investigation reveals mayor received kickbacks from construction companies for city contracts...',\n",
    "            'Former trader reveals how major banks manipulate stock prices through coordinated trading...'\n",
    "        ],\n",
    "        'label': [1, 1, 1, 0, 0]  # 1 = fake, 0 = real\n",
    "    }\n",
    "\n",
    "    sentiment_data = {\n",
    "        'text': [\n",
    "            'I absolutely love this new technology! It\\'s amazing!',\n",
    "            'This product is terrible, waste of money.',\n",
    "            'The weather is okay today, nothing special.',\n",
    "            'Outstanding performance by the team today!',\n",
    "            'Not sure how I feel about this decision.'\n",
    "        ],\n",
    "        'sentiment': ['positive', 'negative', 'neutral', 'positive', 'neutral']\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(fake_news_data), pd.DataFrame(sentiment_data)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    if not os.path.exists('WELFake_Dataset.csv'):\n",
    "        !wget -q https://zenodo.org/record/4561253/files/WELFake_Dataset.csv\n",
    "\n",
    "    if not os.path.exists('train.csv'):\n",
    "        !wget -q https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv\n",
    "\n",
    "    fake_news_df = pd.read_csv('WELFake_Dataset.csv')\n",
    "    sentiment_df = pd.read_csv('train.csv', encoding='latin-1', header=None)\n",
    "    sentiment_df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "    print(f\"Loaded {len(fake_news_df)} fake news samples and {len(sentiment_df)} sentiment samples\")\n",
    "except Exception:\n",
    "    print(\"Using sample data for demonstration...\")\n",
    "    fake_news_df, sentiment_df = create_sample_data()\n",
    "    print(f\"Created {len(fake_news_df)} fake news samples and {len(sentiment_df)} sentiment samples\")\n",
    "\n",
    "# Text Preprocessing Pipeline\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'http[s]?://\\S+', '', text)\n",
    "        text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "        text = re.sub(r'#[A-Za-z0-9_]+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [token for token in tokens if token not in self.stop_words and len(token) > 2]\n",
    "\n",
    "    def lemmatize_tokens(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    def stem_tokens(self, tokens):\n",
    "        return [self.stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    def extract_named_entities(self, text):\n",
    "        doc = self.nlp(text)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        return entities\n",
    "\n",
    "    def preprocess_text(self, text, include_entities=True, use_lemmatization=True):\n",
    "        cleaned = self.clean_text(text)\n",
    "        tokens = self.tokenize_text(cleaned)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.lemmatize_tokens(tokens) if use_lemmatization else self.stem_tokens(tokens)\n",
    "\n",
    "        entities = self.extract_named_entities(cleaned) if include_entities else []\n",
    "\n",
    "        return {\n",
    "            'processed_text': ' '.join(tokens),\n",
    "            'tokens': tokens,\n",
    "            'entities': entities,\n",
    "            'original_length': len(text.split()),\n",
    "            'processed_length': len(tokens),\n",
    "        }\n",
    "\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Apply preprocessing to datasets\n",
    "print(\"Applying preprocessing to datasets...\")\n",
    "\n",
    "if 'text' in fake_news_df.columns:\n",
    "    fake_news_df['processed_text'] = fake_news_df['text'].apply(\n",
    "        lambda x: preprocessor.preprocess_text(str(x))['processed_text']\n",
    "    )\n",
    "\n",
    "sentiment_df['processed_text'] = sentiment_df['text'].apply(\n",
    "    lambda x: preprocessor.preprocess_text(str(x))['processed_text']\n",
    ")\n",
    "\n",
    "# Neural Network Architectures\n",
    "class NeuralArchitectures:\n",
    "    def __init__(self, vocab_size=10000, max_length=100, embedding_dim=128):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def create_lstm_model(self, num_classes=3, lstm_units=64):\n",
    "        model = Sequential([\n",
    "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
    "            LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def create_bilstm_model(self, num_classes=3, lstm_units=64):\n",
    "        model = Sequential([\n",
    "            Embedding(self.vocab_size, self.embedding_dim, input_length=self.max_length),\n",
    "            Bidirectional(LSTM(lstm_units, dropout=0.3, recurrent_dropout=0.3)),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.5),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def create_cnn_model(self, num_classes=3, filters=64, kernel_sizes=[3,4,5]):\n",
    "        inputs = Input(shape=(self.max_length,))\n",
    "        embedding = Embedding(self.vocab_size, self.embedding_dim)(inputs)\n",
    "        conv_blocks = []\n",
    "        for kernel_size in kernel_sizes:\n",
    "            conv = Conv1D(filters, kernel_size, activation='relu')(embedding)\n",
    "            conv = GlobalMaxPooling1D()(conv)\n",
    "            conv_blocks.append(conv)\n",
    "        concat = tf.keras.layers.concatenate(conv_blocks)\n",
    "        dense = Dense(128, activation='relu')(concat)\n",
    "        dropout = Dropout(0.5)(dense)\n",
    "        outputs = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(dropout)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "    def create_hybrid_cnn_bilstm_model(self, num_classes=3, cnn_filters=64, lstm_units=64):\n",
    "        inputs = Input(shape=(self.max_length,))\n",
    "        embedding = Embedding(self.vocab_size, self.embedding_dim)(inputs)\n",
    "        conv1 = Conv1D(cnn_filters, 3, activation='relu')(embedding)\n",
    "        conv1 = MaxPooling1D(2)(conv1)\n",
    "        conv2 = Conv1D(cnn_filters, 4, activation='relu')(conv1)\n",
    "        conv2 = MaxPooling1D(2)(conv2)\n",
    "        lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, dropout=0.3))(conv2)\n",
    "        lstm = Bidirectional(LSTM(lstm_units//2, dropout=0.3))(lstm)\n",
    "        dense = Dense(128, activation='relu')(lstm)\n",
    "        dropout = Dropout(0.5)(dense)\n",
    "        dense2 = Dense(64, activation='relu')(dropout)\n",
    "        dropout2 = Dropout(0.3)(dense2)\n",
    "        outputs = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(dropout2)\n",
    "        model = Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "\n",
    "\n",
    "# Data Preparation and Feature Engineering\n",
    "class DataPreparation:\n",
    "    def __init__(self, vocab_size=10000, max_length=100):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def prepare_text_data(self, texts, labels=None, fit_tokenizer=True):\n",
    "        if fit_tokenizer or self.tokenizer is None:\n",
    "            self.tokenizer = Tokenizer(num_words=self.vocab_size, oov_token='<OOV>')\n",
    "            self.tokenizer.fit_on_texts(texts)\n",
    "        sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=self.max_length, padding='post', truncating='post')\n",
    "        y = None\n",
    "        if labels is not None:\n",
    "            # Fit LabelEncoder only if it hasn't been fitted yet or if fit_tokenizer is True\n",
    "            if fit_tokenizer or not hasattr(self.label_encoder, 'classes_'):\n",
    "                self.label_encoder.fit(labels)\n",
    "            y = self.label_encoder.transform(labels)\n",
    "        return X, y\n",
    "\n",
    "    def create_train_test_split(self, X, y, test_size=0.2, validation_size=0.1, stratify=None): # Modified to accept stratify\n",
    "        X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42, stratify=stratify # Modified to use stratify argument\n",
    "        )\n",
    "        val_size_adjusted = validation_size / (1 - test_size)\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_temp, y_temp, test_size=val_size_adjusted, random_state=42, stratify=stratify # Modified to use stratify argument\n",
    "        )\n",
    "        return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Data Preparation for Sentiment Analysis\n",
    "data_prep_sentiment = DataPreparation(vocab_size=10000, max_length=150)\n",
    "sentiment_texts = sentiment_df['processed_text'].fillna('').tolist()\n",
    "sentiment_labels = sentiment_df['sentiment'].tolist() if 'sentiment' in sentiment_df.columns else ['neutral'] * len(sentiment_texts)\n",
    "X_sentiment, y_sentiment = data_prep_sentiment.prepare_text_data(sentiment_texts, sentiment_labels, fit_tokenizer=True)\n",
    "\n",
    "# Check if using sample data and adjust stratify accordingly\n",
    "if len(sentiment_df) <= 5: # Assuming sample data has 5 rows or less\n",
    "    X_train_sent, X_val_sent, X_test_sent, y_train_sent, y_val_sent, y_test_sent = data_prep_sentiment.create_train_test_split(\n",
    "        X_sentiment, y_sentiment, stratify=None # Set stratify to None for sample data\n",
    "    )\n",
    "else:\n",
    "     X_train_sent, X_val_sent, X_test_sent, y_train_sent, y_val_sent, y_test_sent = data_prep_sentiment.create_train_test_split(\n",
    "        X_sentiment, y_sentiment, stratify=y_sentiment # Use stratify for larger datasets\n",
    "    )\n",
    "\n",
    "\n",
    "# Data Preparation for Fake News Detection\n",
    "data_prep_fake = DataPreparation(vocab_size=data_prep_sentiment.vocab_size, max_length=data_prep_sentiment.max_length)\n",
    "data_prep_fake.tokenizer = data_prep_sentiment.tokenizer # Use the same tokenizer\n",
    "if 'processed_text' in fake_news_df.columns:\n",
    "    fake_texts = fake_news_df['processed_text'].fillna('').tolist()\n",
    "    fake_labels = fake_news_df['label'].tolist() if 'label' in fake_news_df.columns else [0] * len(fake_texts)\n",
    "\n",
    "    X_fake, y_fake = data_prep_fake.prepare_text_data(fake_texts, fake_labels, fit_tokenizer=False)\n",
    "\n",
    "    # Check if using sample data and adjust stratify accordingly\n",
    "    if len(fake_news_df) <= 5: # Assuming sample data has 5 rows or less\n",
    "        X_train_fake, X_val_fake, X_test_fake, y_train_fake, y_val_fake, y_test_fake = data_prep_fake.create_train_test_split(\n",
    "            X_fake, y_fake, stratify=None # Set stratify to None for sample data\n",
    "        )\n",
    "    else:\n",
    "        X_train_fake, X_val_fake, X_test_fake, y_train_fake, y_val_fake, y_test_fake = data_prep_fake.create_train_test_split(\n",
    "            X_fake, y_fake, stratify=y_fake # Use stratify for larger datasets\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(\"Fake news data not available, using sample data\")\n",
    "    X_train_fake = X_val_fake = X_test_fake = np.array([[1, 2, 3]])\n",
    "    y_train_fake = y_val_fake = y_test_fake = np.array([0])\n",
    "\n",
    "\n",
    "print(f\"Sentiment data - Train: {X_train_sent.shape[0]}, Val: {X_val_sent.shape[0]}, Test: {X_test_sent.shape[0]}\")\n",
    "print(f\"Fake news data - Train: {X_train_fake.shape[0]}, Val: {X_val_fake.shape[0]}, Test: {X_test_fake.shape[0]}\")\n",
    "print(f\"Vocabulary size: {len(data_prep_sentiment.tokenizer.word_index)}\")\n",
    "print(f\"Sequence length: {data_prep_sentiment.max_length}\")\n",
    "\n",
    "# Model Training with Callbacks\n",
    "class ModelTrainer:\n",
    "    def __init__(self):\n",
    "        self.history = {}\n",
    "        self.trained_models = {}\n",
    "\n",
    "    def get_callbacks(self, patience=5, min_delta=0.001):\n",
    "        return [\n",
    "            EarlyStopping(monitor='val_loss', patience=patience, min_delta=min_delta, restore_best_weights=True, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "        ]\n",
    "\n",
    "    def train_model(self, model, X_train, y_train, X_val, y_val, model_name='model', epochs=50, batch_size=32):\n",
    "        print(f\"Training {model_name}...\")\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        loss_fn = 'binary_crossentropy' if num_classes == 2 else 'sparse_categorical_crossentropy'\n",
    "        metrics = ['accuracy']\n",
    "        model.compile(optimizer='adam', loss=loss_fn, metrics=metrics)\n",
    "\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=self.get_callbacks(),\n",
    "            verbose=1\n",
    "        )\n",
    "        self.history[model_name] = history\n",
    "        self.trained_models[model_name] = model\n",
    "        print(f\"{model_name} training completed!\")\n",
    "        return history\n",
    "\n",
    "    def plot_training_history(self, model_names=None):\n",
    "        if model_names is None:\n",
    "            model_names = list(self.history.keys())\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "\n",
    "        for name in model_names:\n",
    "            h = self.history[name]\n",
    "            axs[0].plot(h.history['accuracy'], label=f\"{name} Train\")\n",
    "            axs[0].plot(h.history['val_accuracy'], label=f\"{name} Val\")\n",
    "            axs[1].plot(h.history['loss'], label=f\"{name} Train\")\n",
    "            axs[1].plot(h.history['val_loss'], label=f\"{name} Val\")\n",
    "        axs[0].set_title(\"Accuracy\")\n",
    "        axs[0].legend()\n",
    "        axs[1].set_title(\"Loss\")\n",
    "        axs[1].legend()\n",
    "        plt.show()\n",
    "\n",
    "trainer = ModelTrainer()\n",
    "nn_architectures_sentiment = NeuralArchitectures(vocab_size=data_prep_sentiment.vocab_size, max_length=data_prep_sentiment.max_length)\n",
    "nn_architectures_fake = NeuralArchitectures(vocab_size=data_prep_fake.vocab_size, max_length=data_prep_fake.max_length)\n",
    "\n",
    "\n",
    "# Train sentiment models (Example: LSTM, BiLSTM, Hybrid CNN-BiLSTM)\n",
    "lstm_sentiment = nn_architectures_sentiment.create_lstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
    "trainer.train_model(lstm_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='LSTM_Sentiment', epochs=10, batch_size=32)\n",
    "\n",
    "bilstm_sentiment = nn_architectures_sentiment.create_bilstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
    "trainer.train_model(bilstm_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='BiLSTM_Sentiment', epochs=10, batch_size=32)\n",
    "\n",
    "hybrid_sentiment = nn_architectures_sentiment.create_hybrid_cnn_bilstm_model(num_classes=len(data_prep_sentiment.label_encoder.classes_))\n",
    "trainer.train_model(hybrid_sentiment, X_train_sent, y_train_sent, X_val_sent, y_val_sent, model_name='Hybrid_CNN_BiLSTM', epochs=10, batch_size=32)\n",
    "\n",
    "trainer.plot_training_history(model_names=['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM'])\n",
    "\n",
    "# Train fake news models (Example: LSTM, BiLSTM, Hybrid CNN-BiLSTM)\n",
    "lstm_fake = nn_architectures_fake.create_lstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
    "trainer.train_model(lstm_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='LSTM_Fake', epochs=10, batch_size=32)\n",
    "\n",
    "bilstm_fake = nn_architectures_fake.create_bilstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
    "trainer.train_model(bilstm_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='BiLSTM_Fake', epochs=10, batch_size=32)\n",
    "\n",
    "hybrid_fake = nn_architectures_fake.create_hybrid_cnn_bilstm_model(num_classes=len(data_prep_fake.label_encoder.classes_))\n",
    "trainer.train_model(hybrid_fake, X_train_fake, y_train_fake, X_val_fake, y_val_fake, model_name='Hybrid_CNN_BiLSTM_Fake', epochs=10, batch_size=32)\n",
    "\n",
    "trainer.plot_training_history(model_names=['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake'])\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "class ModelEvaluator:\n",
    "    def __init__(self):\n",
    "        self.evaluation_results = {}\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test, model_name='model'):\n",
    "        print(f\"Evaluating {model_name}...\")\n",
    "        y_pred_proba = model.predict(X_test, verbose=0)\n",
    "        if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:\n",
    "            y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "            is_binary = False\n",
    "        else:\n",
    "            y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "            is_binary = True\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        if is_binary:\n",
    "            precision = precision_score(y_test, y_pred)\n",
    "            recall = recall_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "        else:\n",
    "            precision = precision_score(y_test, y_pred, average='weighted')\n",
    "            recall = recall_score(y_test, y_pred, average='weighted')\n",
    "            f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "        self.evaluation_results[model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'y_true': y_test,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba,\n",
    "        }\n",
    "        print(f\"{model_name} evaluation completed!\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "        return self.evaluation_results[model_name]\n",
    "\n",
    "    def plot_confusion_matrices(self, model_names=None):\n",
    "        if model_names is None:\n",
    "            model_names = list(self.evaluation_results.keys())\n",
    "        n = len(model_names)\n",
    "        fig, axs = plt.subplots(1, n, figsize=(5*n, 4))\n",
    "        if n == 1:\n",
    "            axs = [axs]\n",
    "        for i, name in enumerate(model_names):\n",
    "            results = self.evaluation_results[name]\n",
    "            cm = confusion_matrix(results['y_true'], results['y_pred'])\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axs[i])\n",
    "            axs[i].set_title(f\"{name} Confusion Matrix\")\n",
    "            axs[i].set_xlabel(\"Predicted\")\n",
    "            axs[i].set_ylabel(\"Actual\")\n",
    "        plt.show()\n",
    "\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate sentiment models\n",
    "for model_name in ['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM']:\n",
    "    evaluator.evaluate_model(trainer.trained_models[model_name], X_test_sent, y_test_sent, model_name)\n",
    "evaluator.plot_confusion_matrices(model_names=['LSTM_Sentiment', 'BiLSTM_Sentiment', 'Hybrid_CNN_BiLSTM'])\n",
    "\n",
    "# Evaluate fake news models\n",
    "for model_name in ['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake']:\n",
    "    evaluator.evaluate_model(trainer.trained_models[model_name], X_test_fake, y_test_fake, model_name)\n",
    "evaluator.plot_confusion_matrices(model_names=['LSTM_Fake', 'BiLSTM_Fake', 'Hybrid_CNN_BiLSTM_Fake'])\n",
    "\n",
    "\n",
    "# Explainable AI Implementation\n",
    "class ExplainableAI:\n",
    "    def __init__(self, tokenizer, label_encoder=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.lime_explainer = LimeTextExplainer(class_names=label_encoder.classes_ if label_encoder is not None else ['negative', 'neutral', 'positive'])\n",
    "\n",
    "    def create_prediction_function(self, model, max_length=150):\n",
    "        def predict_fn(texts):\n",
    "            sequences = self.tokenizer.texts_to_sequences(texts)\n",
    "            padded = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "            preds = model.predict(padded, verbose=0)\n",
    "            if len(preds.shape) == 1:\n",
    "                proba = np.array([1 - preds, preds]).T\n",
    "            else:\n",
    "                proba = preds\n",
    "            return proba\n",
    "        return predict_fn\n",
    "\n",
    "    def explain_prediction_lime(self, model, text, num_features=10, max_length=150):\n",
    "        # Re-initialize explainer with correct class names based on the model's task\n",
    "        class_names = self.label_encoder.classes_.tolist() if self.label_encoder is not None else ['negative', 'neutral', 'positive']\n",
    "        self.lime_explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "        predict_fn = self.create_prediction_function(model, max_length)\n",
    "        explanation = self.lime_explainer.explain_instance(text, predict_fn, num_features=num_features)\n",
    "        return explanation\n",
    "\n",
    "    def visualize_lime_explanation(self, explanation, save_path=None):\n",
    "        exp_list = explanation.as_list()\n",
    "        words = [item[0] for item in exp_list]\n",
    "        weights = [item[1] for item in exp_list]\n",
    "        colors = ['red' if w < 0 else 'green' for w in weights]\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        bars = plt.barh(words, weights, color=colors, alpha=0.7)\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.title('LIME Explanation - Word Importance')\n",
    "        plt.axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "\n",
    "        for bar, weight in zip(bars, weights):\n",
    "            plt.text(weight + (0.01 if weight >= 0 else -0.01), bar.get_y() + bar.get_height()/2,\n",
    "                     f'{weight:.3f}', ha='left' if weight >= 0 else 'right', va='center')\n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Explainable AI for Sentiment Analysis\n",
    "explainer_sentiment = ExplainableAI(data_prep_sentiment.tokenizer, data_prep_sentiment.label_encoder)\n",
    "\n",
    "# Example usage for explanation for sentiment analysis\n",
    "sample_text_sentiment = \"The government has announced a new policy that positively impacts economic growth.\"\n",
    "sample_processed_sentiment = preprocessor.preprocess_text(sample_text_sentiment)['processed_text']\n",
    "explanation_sentiment = explainer_sentiment.explain_prediction_lime(trainer.trained_models['Hybrid_CNN_BiLSTM'], sample_processed_sentiment)\n",
    "explainer_sentiment.visualize_lime_explanation(explanation_sentiment)\n",
    "\n",
    "# Explainable AI for Fake News Detection\n",
    "explainer_fake = ExplainableAI(data_prep_fake.tokenizer, data_prep_fake.label_encoder)\n",
    "\n",
    "# Example usage for explanation for fake news detection\n",
    "sample_text_fake = \"Breaking: Scientists discover cure for all diseases\"\n",
    "sample_processed_fake = preprocessor.preprocess_text(sample_text_fake)['processed_text']\n",
    "explanation_fake = explainer_fake.explain_prediction_lime(trainer.trained_models['Hybrid_CNN_BiLSTM_Fake'], sample_processed_fake)\n",
    "explainer_fake.visualize_lime_explanation(explanation_fake)\n",
    "\n",
    "\n",
    "# Continuous Learning Mechanism\n",
    "class ContinuousLearning:\n",
    "    def __init__(self, base_model, tokenizer, label_encoder, learning_rate=0.0001):\n",
    "        self.base_model = base_model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.learning_rate = learning_rate\n",
    "        self.update_history = []\n",
    "        self.performance_tracking = []\n",
    "\n",
    "    def incremental_update(self, new_texts, new_labels, epochs=3, batch_size=16, validation_split=0.2):\n",
    "        print(f\"Performing incremental update with {len(new_texts)} new samples...\")\n",
    "        sequences = self.tokenizer.texts_to_sequences(new_texts)\n",
    "        X_new = pad_sequences(sequences, maxlen=150, padding='post', truncating='post')\n",
    "        y_new = self.label_encoder.transform(new_labels)\n",
    "        self.base_model.optimizer.learning_rate = self.learning_rate\n",
    "        history = self.base_model.fit(\n",
    "            X_new, y_new,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=validation_split,\n",
    "            verbose=1\n",
    "        )\n",
    "        update_info = {\n",
    "            'timestamp': pd.Timestamp.now(),\n",
    "            'num_samples': len(new_texts),\n",
    "            'final_loss': history.history['loss'][-1],\n",
    "            'final_accuracy': history.history['accuracy'][-1],\n",
    "        }\n",
    "        self.update_history.append(update_info)\n",
    "        print(\"Incremental update completed!\")\n",
    "        print(f\"Final Loss: {update_info['final_loss']:.4f}\")\n",
    "        print(f\"Final Accuracy: {update_info['final_accuracy']:.4f}\")\n",
    "        return history\n",
    "\n",
    "    def evaluate_performance_drift(self, X_test, y_test, threshold=0.05):\n",
    "        y_pred = np.argmax(self.base_model.predict(X_test, verbose=0), axis=1)\n",
    "        current_accuracy = accuracy_score(y_test, y_pred)\n",
    "        performance_info = {'timestamp': pd.Timestamp.now(), 'accuracy': current_accuracy}\n",
    "        self.performance_tracking.append(performance_info)\n",
    "        if len(self.performance_tracking) > 1:\n",
    "            previous_accuracy = self.performance_tracking[-2]['accuracy']\n",
    "            drift = abs(current_accuracy - previous_accuracy)\n",
    "            if drift > threshold:\n",
    "                print(f\"Performance drift detected: {drift:.4f}\")\n",
    "                return True, drift\n",
    "            else:\n",
    "                print(f\"Performance stable: {drift:.4f}\")\n",
    "                return False, drift\n",
    "        return False, 0.0\n",
    "\n",
    "# Continuous Learning for Sentiment Analysis\n",
    "continuous_learner_sentiment = ContinuousLearning(base_model=trainer.trained_models['Hybrid_CNN_BiLSTM'], tokenizer=data_prep_sentiment.tokenizer, label_encoder=data_prep_sentiment.label_encoder)\n",
    "\n",
    "# Example incremental learning simulation (with dummy new data for sentiment analysis)\n",
    "new_texts_sentiment = [\n",
    "    \"The economy is improving steadily with new policies.\",\n",
    "    \"Many people are skeptical about the government's new plan.\",\n",
    "    \"This is a bad decision and hurts citizens greatly.\"\n",
    "]\n",
    "new_processed_sentiment = [preprocessor.preprocess_text(text)['processed_text'] for text in new_texts_sentiment]\n",
    "new_labels_sentiment = ['positive', 'neutral', 'negative']\n",
    "\n",
    "continuous_learner_sentiment.incremental_update(new_processed_sentiment, new_labels_sentiment, epochs=2)\n",
    "\n",
    "# Continuous Learning for Fake News Detection\n",
    "continuous_learner_fake = ContinuousLearning(base_model=trainer.trained_models['Hybrid_CNN_BiLSTM_Fake'], tokenizer=data_prep_fake.tokenizer, label_encoder=data_prep_fake.label_encoder)\n",
    "\n",
    "# Example incremental learning simulation (with dummy new data for fake news detection)\n",
    "new_texts_fake = [\n",
    "    \"New study shows coffee cures cancer\",\n",
    "    \"Local hero saves cat from tree\",\n",
    "    \"Politician resigns amid scandal\"\n",
    "]\n",
    "new_processed_fake = [preprocessor.preprocess_text(text)['processed_text'] for text in new_texts_fake]\n",
    "new_labels_fake = [1, 0, 0] # 1 = fake, 0 = real\n",
    "\n",
    "continuous_learner_fake.incremental_update(new_processed_fake, new_labels_fake, epochs=2)\n",
    "\n",
    "\n",
    "print(\"Complete NLP system source code ready for execution in Google Colab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
